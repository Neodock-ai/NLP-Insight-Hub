import re
import logging

# Configure module logger
logger = logging.getLogger(__name__)

def clean_text(text):
    """
    Enhanced text cleaning and normalization for better NLP performance.
    
    Args:
        text (str): The raw input text
        
    Returns:
        str: Cleaned and normalized text
    """
    try:
        if not text:
            logger.warning("Empty text received for cleaning")
            return ""
        if not isinstance(text, str):
            text = str(text)

        cleaned = re.sub(r'\s+', ' ', text)
        cleaned = re.sub(r'[^\w\s.,;:!?"\'()\[\]{}*&%$#@-]', '', cleaned)
        cleaned = _fix_common_ocr_errors(cleaned)
        cleaned = _fix_sentence_spacing(cleaned)
        cleaned = _remove_duplicate_sentences(cleaned)
        cleaned = _normalize_quotes(cleaned)

        return cleaned.strip()

    except Exception as e:
        logger.error(f"Error during text cleaning: {str(e)}")
        return text.strip() if isinstance(text, str) else str(text).strip()

def preprocess_for_summarization(text):
    try:
        cleaned = clean_text(text)
        cleaned = _remove_boilerplate(cleaned)
        cleaned = _normalize_paragraphs(cleaned)
        if len(cleaned) > 10000:
            logger.info("Long text detected, truncating for summarization")
            cleaned = cleaned[:6000] + "\n\n[...]\n\n" + cleaned[-4000:]
        return cleaned
    except Exception as e:
        logger.error(f"Error during summarization preprocessing: {str(e)}")
        return clean_text(text)

def preprocess_for_sentiment(text):
    try:
        cleaned = clean_text(text)
        cleaned = _normalize_emoticons(cleaned)
        cleaned = _normalize_emphasis(cleaned)
        if len(cleaned) > 5000:
            logger.info("Long text detected, sampling for sentiment analysis")
            cleaned = _strategic_sample_for_sentiment(cleaned)
        return cleaned
    except Exception as e:
        logger.error(f"Error during sentiment preprocessing: {str(e)}")
        return clean_text(text)

def preprocess_for_keywords(text):
    try:
        cleaned = clean_text(text)
        cleaned = _remove_headers_footers(cleaned)
        if len(cleaned) > 10000:
            logger.info("Long text detected, sampling for keyword extraction")
            cleaned = _strategic_sample_for_keywords(cleaned)
        return cleaned
    except Exception as e:
        logger.error(f"Error during keyword preprocessing: {str(e)}")
        return clean_text(text)

def _fix_common_ocr_errors(text):
    replacements = [
        (r'l\\b', 'i'),
        (r'\\bl\\b', 'I'),
        (r'0', 'o'),
        (r'1', 'l'),
        (r'S', '5'),
        (r'rn', 'm'),
        (r'IVI', 'M'),
        (r'\\bI\\s+([a-z])', r'I \\1'),
    ]
    ocr_indicators = ['l l', 'l l l', '0n', 'c0m', 'f0r', '0f', 'l1fe', 'lS', 'l5']
    if any(indicator in text for indicator in ocr_indicators):
        for pattern, replacement in replacements:
            text = re.sub(pattern, replacement, text)
    return text

def _fix_sentence_spacing(text):
    text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)
    text = re.sub(r',([A-Za-z])', r', \\1', text)
    return text

def _remove_duplicate_sentences(text):
    sentences = re.split(r'(?<=[.!?])\\s+', text)
    unique_sentences = []
    seen = set()
    for sentence in sentences:
        normalized = re.sub(r'\\s+', ' ', sentence.lower().strip())
        if len(normalized) > 20 and normalized not in seen:
            seen.add(normalized)
            unique_sentences.append(sentence)
        elif len(normalized) <= 20:
            unique_sentences.append(sentence)
    return ' '.join(unique_sentences)

def _normalize_quotes(text):
    text = re.sub(r'[“”]', '"', text)
    text = re.sub(r'[‘’]', "'", text)
    return text

def _remove_boilerplate(text):
    patterns = [
        r'(?i)all rights reserved\\.?',
        r'(?i)confidential(?:ity)? notice:?.*?$',
        r'(?i)copyright ©.*?(?:\\d{4}|\\[year\\]).*?$',
        r'(?i)this (?:document|email) (?:is|contains).*?confidential.*?$',
        r'(?i)disclaimer:.*?$',
        r'(?i)legal notice:.*?$',
        r'(?i)produced by.*?$',
        r'(?i)generated by.*?$',
        r'(?i)page \\d+ of \\d+',
        r'(?i)https?://(?:www\\.)?[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+/?[^\\s]*'
    ]
    for pattern in patterns:
        text = re.sub(pattern, '', text)
    return text

def _normalize_paragraphs(text):
    text = re.sub(r'\\n{3,}', '\\n\\n', text)
    text = re.sub(r'([.!?])\\s*(\\n?)([A-Z])', r'\\1\\n\\n\\3', text)
    return text

def _normalize_emoticons(text):
    emoticons = {
        r':-?\\)': ' [POSITIVE_EMOJI] ',
        r':-?\\(': ' [NEGATIVE_EMOJI] ',
        r':-?D': ' [VERY_POSITIVE_EMOJI] ',
        r':-?P': ' [PLAYFUL_EMOJI] ',
        r':-?/': ' [SKEPTICAL_EMOJI] ',
        r':-?\\|': ' [NEUTRAL_EMOJI] ',
        r';-?\\)': ' [WINK_EMOJI] ',
    }
    for pattern, replacement in emoticons.items():
        text = re.sub(pattern, replacement, text)
    return text

def _normalize_emphasis(text):
    def mark_caps(match):
        text = match.group(0)
        return f" [EMPHASIS] {text.lower()} [/EMPHASIS] " if len(text) >= 3 else text
    text = re.sub(r'\\b[A-Z]{3,}\\b', mark_caps, text)
    text = re.sub(r'!{2,}', ' [STRONG_EMPHASIS] ! ', text)
    text = re.sub(r'\\?{2,}', ' [STRONG_QUESTION] ? ', text)
    return text

def _strategic_sample_for_sentiment(text):
    paragraphs = text.split('\\n\\n')
    if len(paragraphs) <= 5:
        return text
    result = '\\n\\n'.join(paragraphs[:2]) + '\\n\\n'
    middle_start = max(2, len(paragraphs) // 4)
    middle_end = min(len(paragraphs) - 2, 3 * len(paragraphs) // 4)
    sample_count = min(4, middle_end - middle_start)
    if sample_count > 0:
        step_size = max(1, (middle_end - middle_start) // sample_count)
        for i in range(middle_start, middle_end, step_size):
            if i < len(paragraphs):
                result += paragraphs[i] + '\\n\\n'
    result += '\\n\\n' + '\\n\\n'.join(paragraphs[-2:])
    return result

def _remove_headers_footers(text):
    patterns = [
        r'^\\s*\\d+\\s*$',
        r'^\\s*Page \\d+ of \\d+\\s*$',
        r'^\\s*[^a-zA-Z]*\\d+[^a-zA-Z]*\\s*$',
        r'^\\s*[A-Za-z0-9_.-]+@[A-Za-z0-9_.-]+\\.\\w+\\s*$',
        r'^\\s*https?://\\S+\\s*$',
        r'^\\s*www\\.\\S+\\s*$',
        r'^\\s*[A-Za-z\\s]+ \\| [A-Za-z\\s]+\\s*$',
        r'^\\s*CONFIDENTIAL\\s*$',
        r'^\\s*DRAFT\\s*$',
    ]
    lines = text.split('\\n')
    filtered_lines = []
    for line in lines:
        if not any(re.match(pattern, line) for pattern in patterns):
            filtered_lines.append(line)
    return '\\n'.join(filtered_lines)

def _strategic_sample_for_keywords(text):
    headings = re.findall(r'(?m)^[A-Z][\\w\\s:]{3,60}(?<!\\.)', text)
    paragraphs = text.split('\\n\\n')
    first_sentences = [re.split(r'(?<=[.!?])\\s+', para)[0] for para in paragraphs if para]
    sample = ""
    if headings:
        sample += "HEADINGS:\\n" + "\\n".join(headings) + "\\n\\n"
    if first_sentences:
        sample += "KEY SENTENCES:\\n" + " ".join(first_sentences[:15]) + "\\n\\n"
    sample += "BEGINNING:\\n" + text[:2000].strip() + "\\n\\n"
    sample += "END:\\n" + text[-2000:].strip()
    return sample
